### 예측하고 비교하고 학습하라
    - 비교 : 가중치를 어떻게 설정하면 신경망이 정교해질까?
        - 얼마나 잘 예측했는지 평가 : 오차
        - 평균제곱오차 : 큰 실수는 과장 작은 실수는 무시
    - 학습 : 오차를 줄이기 위하여 가중치 수정, 오차를 0으로 만들기
        - 온냉학습 : 가중치를 좌우로 흔들어 가장 많이 줄이는 방향으로 가중치 이동하여 0 이될때까지 반복
            ex) 입력(0.5), 목표(0.8) -> 가중치(0.5) -> 예측결과(0.25), 오차(0.30)
            비효율적, 이동 방향과 거리를 알수 없음
        - 경사하강법
            순오차(델타) : 방향과 거리 데이터, +(높게예측), -(낮게예측)
            종료 : 순오차에 입력을 곱게 0이 아닌 오차로 표시
            음의 반전 : 입력이 음수일경우 순오차와 반대로 됨
            스케일링 : 순오차에 입력을 곱하면 커지는 부작용 => 알파 사용
            오차 = (예측치 - 목표예측치) ** 2 = ((입력 X 가중치) - 목표예측치) ** 2
            미분계수(weight_delta) = 순오차(델타) * 입력                          
            가중치 = 가중치 - 알파 X 미분계수 : 스케일링 방지
        



```python
# 비교 : 온냉학습
goal_pred = 0.8       #목표 예측치
input = 0.5           #입력
weight = 0.5          #가중치
step_amount = 0.001   #반복마다 수정할 가중치 크기

for iteration in range(1101):
    pred = input * weight   #예측치
    error = (pred - goal_pred) ** 2 #오차 : 평균제곱오차

    print("Pred : ", pred, "Error : ", error)
    
    up_pred = input * (weight + step_amount)     #가중치를 올리기
    up_error = (goal_pred - up_pred) ** 2        #오차 : 평균제곱오차
    
    down_pred = input * (weight - step_amount)     #가중치를 내리기
    down_error = (goal_pred - down_pred) ** 2        #오차 : 평균제곱오차
    
    if(down_error < up_error):
        weight = weight - step_amount
    
    if(down_error > up_error):
        weight = weight + step_amount
        
```

    Pred :  0.25 Error :  0.30250000000000005
    Pred :  0.2505 Error :  0.3019502500000001
    Pred :  0.251 Error :  0.30140100000000003
    Pred :  0.2515 Error :  0.30085225
    


```python
# 오차 : 방향과 거리 계산
goal_pred = 0.8       #목표 예측치
input = 0.5           #입력
weight = 0.5          #가중치

weight, goal_pred, input = (0.0, 0.8, 1.1)
for iteration in range(4):
    pred = input * weight                                 #예측치
    error = (pred - goal_pred) ** 2                       #오차 : 평균제곱오차, 순오차 : pred - goal_pred
    direction_and_amount = (pred - goal_pred) * input     #오차의 방행과 거리
    
    delta = pred - goal_pred                              #순오차
    weight_delta = delta * input                          #미분계수
    
    #weight = weight - direction_and_amount                #가중치 수정
    weight = weight - weight_delta                         #가중치 수정

    print("Pred : ", pred, "Error : ", error, "Direct_Amount : ", direction_and_amount, "Weight : ", weight)
    print("Delta : ", delta, "Weight_delta : ", weight_delta)
    
    
```

    Pred :  0.0 Error :  0.6400000000000001 Direct_Amount :  -0.8800000000000001 Weight :  0.8800000000000001
    Delta :  -0.8 Weight_delta :  -0.8800000000000001
    Pred :  0.9680000000000002 Error :  0.02822400000000005 Direct_Amount :  0.1848000000000002 Weight :  0.6951999999999999
    Delta :  0.16800000000000015 Weight_delta :  0.1848000000000002
    Pred :  0.76472 Error :  0.0012446784000000064 Direct_Amount :  -0.0388080000000001 Weight :  0.734008
    Delta :  -0.03528000000000009 Weight_delta :  -0.0388080000000001
    Pred :  0.8074088 Error :  5.4890317439999896e-05 Direct_Amount :  0.008149679999999992 Weight :  0.72585832
    Delta :  0.007408799999999993 Weight_delta :  0.008149679999999992
    


```python
# 알파 * 미분계수

weight, goal_pred, input, alpha = (0.5, 0.8, 2, 0.1)

for iteration in range(20):
    pred = input * weight                                 #예측치
    error = (pred - goal_pred) ** 2                       #오차 : 평균제곱오차, 순오차 : pred - goal_pred
        
    delta = pred - goal_pred                              #순오차
    weight_delta = delta * input                          #미분계수
    
    #weight = weight - direction_and_amount                #가중치 수정
    weight = weight - (alpha * weight_delta)               #알파 가중치 수정

    print("Pred : ", pred, "Error : ", error, "Weight : ", weight)
    
```

    Pred :  1.0 Error :  0.03999999999999998 Weight :  0.46
    Pred :  0.92 Error :  0.0144 Weight :  0.436
    Pred :  0.872 Error :  0.005183999999999993 Weight :  0.42160000000000003
    Pred :  0.8432000000000001 Error :  0.0018662400000000014 Weight :  0.41296000000000005
    Pred :  0.8259200000000001 Error :  0.0006718464000000028 Weight :  0.407776
    Pred :  0.815552 Error :  0.00024186470400000033 Weight :  0.4046656
    Pred :  0.8093312 Error :  8.70712934399997e-05 Weight :  0.40279936
    Pred :  0.80559872 Error :  3.134566563839939e-05 Weight :  0.401679616
    Pred :  0.803359232 Error :  1.1284439629823931e-05 Weight :  0.4010077696
    Pred :  0.8020155392 Error :  4.062398266736526e-06 Weight :  0.40060466176000004
    Pred :  0.8012093235200001 Error :  1.4624633760252567e-06 Weight :  0.40036279705600003
    Pred :  0.8007255941120001 Error :  5.264868153690924e-07 Weight :  0.40021767823360005
    Pred :  0.8004353564672001 Error :  1.8953525353291194e-07 Weight :  0.40013060694016006
    Pred :  0.8002612138803201 Error :  6.82326912718715e-08 Weight :  0.40007836416409603
    Pred :  0.8001567283281921 Error :  2.456376885786678e-08 Weight :  0.40004701849845764
    Pred :  0.8000940369969153 Error :  8.842956788836216e-09 Weight :  0.4000282110990746
    Pred :  0.8000564221981492 Error :  3.1834644439835434e-09 Weight :  0.40001692665944477
    Pred :  0.8000338533188895 Error :  1.1460471998340758e-09 Weight :  0.40001015599566686
    Pred :  0.8000203119913337 Error :  4.125769919393652e-10 Weight :  0.40000609359740014
    Pred :  0.8000121871948003 Error :  1.485277170987127e-10 Weight :  0.4000036561584401
    
